{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023\n",
    "    Juan López Quirós\n",
    "    Jose Castro Vázquez"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "raw",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "    print(\"ORIGINAL: \\n\")\n",
    "    print(data[25:30])\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "    print(\"GIVE COLUMN NAMES: \\n\")\n",
    "    print(data[25:30])\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We put the cleaned data into a new csv file\n",
    "    data_cleaned = target_filename\n",
    "    data.to_csv(data_cleaned, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data[25:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"tweet_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5700f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
