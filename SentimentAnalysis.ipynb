{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. Para leer el fichero de datos y procesarlo, utilizaremos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e578258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1178fa45",
   "metadata": {},
   "source": [
    "Definimos la función que se encarga de leer y procesar el fichero de datos. Esta función eliminará las columnas irrelevantes, seleccionará un subset de 5000 tweets que formará el corpus de nuestros modelos y por último los guardará en otro fichero para su uso más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    #uft-8 encoding didn't work, latin1 encoding did.\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We select a subset of 5000 tweets\n",
    "    data = data[:5000]\n",
    "\n",
    "    #Save data onto new file\n",
    "    data.to_csv(target_filename, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: \n",
      "\n",
      "                                                   text\n",
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "...                                                 ...\n",
      "4995                                    long day today \n",
      "4996                     a friend broke his promises.. \n",
      "4997               @gjarnling I am fine thanks - tired \n",
      "4998          trying to keep my eyes open..damn baking \n",
      "4999                        why the hell is it snowing \n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"subset_tweet_data.csv\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para limpiar, tokenizar y lematizar nuestro dataset.\n",
    "\n",
    "Para esto, descargamos los recursos necesarios para trabajar con la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803a889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bc7c22",
   "metadata": {},
   "source": [
    "Para asegurarnos de que el recurso que contiene las stopwords se descargó correctamente, los mostramos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"couldn't\", 'here', \"it's\", 'him', \"wasn't\", 'himself', 'have', 'myself', 'again', 'after', 'some', 'own', 'its', 'y', 'does', 'we', 'wasn', 'my', \"wouldn't\", 'at', 'what', 'few', \"haven't\", 'aren', 'doing', 'who', 'then', 'only', 'most', 'each', 'but', 'mustn', 'shouldn', 'while', 'is', 'herself', 'hers', 'your', 'are', 'that', 'too', 'nor', 'did', 'below', 'doesn', 'than', 'they', 'by', 'she', \"should've\", \"weren't\", 'any', 't', 'will', 'her', 'further', 'didn', 'he', 'over', 'because', 'down', 'weren', 'for', 'needn', 'out', 'whom', 'if', \"shouldn't\", 'into', 'all', 'ain', 'in', \"isn't\", 'yours', 'had', 'our', 'haven', 'mightn', \"you'd\", 'above', 'should', 'from', 'theirs', 'being', 'when', \"needn't\", 'until', 'before', 'm', 'or', 'you', 'wouldn', 'am', \"you've\", 'ours', 'these', 'as', 'not', 'this', 'were', 'themselves', 'yourself', 'which', 'same', 'don', 'why', 's', 'it', 'was', 'their', \"doesn't\", \"hasn't\", 'through', 'of', 'so', \"you'll\", \"you're\", 'yourselves', 'and', 'how', \"didn't\", 'd', 'very', 'to', 'both', \"mustn't\", 'ma', \"shan't\", \"aren't\", 'hadn', 've', 'll', 'hasn', 'be', 'there', 'other', 'between', 'under', 'just', 'such', 'me', 'with', 'i', 'o', 'about', 'during', 'been', 'an', 'his', 'against', 'up', 'shan', \"she's\", 'on', \"hadn't\", 'them', \"mightn't\", 'off', 'those', 'once', 'itself', 're', 'isn', 'where', 'now', 'couldn', 'can', 'having', 'no', 'ourselves', 'the', 'a', 'do', 'more', 'has', \"don't\", 'won', \"won't\", \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d6b52c",
   "metadata": {},
   "source": [
    "Definimos la función encargada de limpiar los tweets del dataset que eliminará las menciones, hashtags, urls y otros símbolos extraños contenidos en ellos.\n",
    "\n",
    "Además después de pasar por este filtro, los tokenizamos con TweetTokenizer que a diferencia del tokenizador por defecto de NLTK, word_tokenizer, este si mantiene la coherencia en palabras complejas del inglés.\n",
    "\n",
    "Por ejemplo la palabra \"can't\":\n",
    "- Con word_tokenizer-> can't = [\"ca\", \"n't\"]\n",
    "- Con TweetTokenize-> can't = [\"can't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fa634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "def clean_text(original_filename, target_filename, with_punctuation=True):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mention_or_hashtag_characters = [\"@\", \"#\"]\n",
    "\n",
    "    tt = TweetTokenizer() #Tokenizer\n",
    "    ps = PorterStemmer() #Stemmer\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = data.iloc[i][\"text\"]\n",
    "\n",
    "        tweet_tokens = tt.tokenize(tweet) #Tokenizing\n",
    "\n",
    "        filtered_tweet_tokens = [token for token in tweet_tokens if \n",
    "                                    token.lower()[0] not in mention_or_hashtag_characters and #Eliminate mentions, hashtags\n",
    "                                    token.lower() not in stop_words and #Eliminate stop words\n",
    "                                    'http' not in token.lower()] #Eliminate urls\n",
    "\n",
    "        if(not with_punctuation):\n",
    "            filtered_tweet_tokens = [token for token in filtered_tweet_tokens if token not in list(punctuation)]\n",
    "\n",
    "        stemmed_tweet_tokens = [ps.stem(token) for token in filtered_tweet_tokens] #Stemming\n",
    "        \n",
    "        data.loc[i,['text']] = ' '.join(str(token) for token in stemmed_tweet_tokens)\n",
    "\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fcdd82c",
   "metadata": {},
   "source": [
    "Hacemos 2 llamadas a la función. La primera para el corpus sea texto con signos de puntuación inclusive y otro con texto puramente alfanumérico, esto lo hacemos así para estudiar el posible impacto que puedan tener estos en los modelos finales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9a83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text('subset_tweet_data.csv', 'tokenized_data.csv')\n",
    "clean_text('subset_tweet_data.csv', 'tokenized_data_no_punctuation.csv', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de10b66a",
   "metadata": {},
   "source": [
    "El siguiente paso consisitirá en utilizar la librería TextBlob para la clasificacion de los tweets en: Muy Feliz, Contento, Neutro, Molesto, Hater.\n",
    "\n",
    "Para ello primero debemos definir los términos y qué consideraremos como 'Neutro', 'Hater', etc..\n",
    "TextBlob mediante su función 'TextBlob.sentiment' nos hace un análisis de sentimientos de cualquier texto en forma de Objeto Sentiment, un objeto que tiene 2 parámetros:\n",
    "\n",
    "    - Polarity con rango [-1,1] que indica la polaridad del sentimiento del texto siendo -1 un texto muy negativo y 1 muy positivo\n",
    "    \n",
    "    - Subjectivity con rango [0,1] que indica la objetividad del texto siendo 0 muy objetivo y cercano a la realidad y 1 siendo muy subjetivo donde se expresan más las opiniones y sentimientos personales del autor.\n",
    "\n",
    "Teniendo en cuenta las limitaciones del procesamiento de análsis de Textblob definiremos:\n",
    "\n",
    "    - Muy Feliz: Polarity=(0.25,1] Subjectivity=[0-0.5)\n",
    "    - Contento: Polarity=(0.25,1] Subjectivity=[0.5-1]\n",
    "    - Neutro: Polarity=(-0.25,0.25] Subjectivity=any\n",
    "    - Hater: Polarity=[-1,-0.25) Subjectivity=[0-0.5)\n",
    "    - Molesto: Polarity=[-1,-0.25) Subjectivity=[0.5-1]\n",
    "    \n",
    "    Esto lo hacemos así ya que consideramos que la principal diferencia entre \"Muy Feliz\"-\"Contento\" y \"Hater\"-\"Molesto\" es que los primeros son opiniones más subjetivas que las segundas. Y por otra parte consideramos que si el texto es muy objetivo decimos que es neutro ya que no expresa sentimiento ninguno de la persona que escribe el tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83070bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def classify_tweets(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    dic_count = dict()\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = TextBlob(str(data.iloc[i][\"text\"]))\n",
    "        sentiment = tweet.sentiment\n",
    "        tag = choose_classification(sentiment)\n",
    "        data.loc[i,['tag']] = tag\n",
    "\n",
    "        if tag not in dic_count:\n",
    "            dic_count[tag] = 1\n",
    "        else:\n",
    "            dic_count[tag] = dic_count[tag] + 1\n",
    "\n",
    "    print(dic_count)\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "\n",
    "def choose_classification(sentiment):\n",
    "    classification = \"Neutral\"\n",
    "    polarity = sentiment.polarity \n",
    "    subjectivity = sentiment.subjectivity \n",
    "    if polarity < -0.25:\n",
    "        classification = \"Molesto\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Hater\"\n",
    "    elif polarity > 0.25:\n",
    "        classification = \"Contento\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Muy Feliz\"\n",
    "    return classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a735976a",
   "metadata": {},
   "source": [
    "Probamos la clasificación de los tweets en los 2 datasets uno con signos de puntuación y otro sin ellos. Podemos ver, por los diccionarios contadores calculados, que devuelven resultados muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2afaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Neutral': 3516, 'Molesto': 656, 'Contento': 596, 'Hater': 80, 'Muy Feliz': 152}\n",
      "{'Neutral': 3467, 'Molesto': 667, 'Contento': 611, 'Hater': 83, 'Muy Feliz': 172}\n"
     ]
    }
   ],
   "source": [
    "classify_tweets(\"tokenized_data_no_punctuation.csv\", \"final_dataset.csv\")\n",
    "classify_tweets(\"tokenized_data.csv\", \"final_dataset_punctuation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb3f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic math algorithms\n",
    "\n",
    "'''def map_algorithm(candidates):\n",
    "\n",
    "  max = {'probability': .0}\n",
    "\n",
    "  for candidate in candidates:\n",
    "    probability = candidate['probability']\n",
    "\n",
    "    if(max['probability'] < probability):\n",
    "      max = candidate\n",
    "\n",
    "  return max'''\n",
    "\n",
    "def basic_distance(tweet1, tweet2):\n",
    "\n",
    "  distance = 0\n",
    "\n",
    "  text1 = tweet1('text')\n",
    "  text2 = tweet2('text')\n",
    "\n",
    "  for word1 in text1:\n",
    "    for word2 in text2:\n",
    "      if word1 != word2:\n",
    "        distance += 1\n",
    "  \n",
    "  return distance\n",
    "\n",
    "def laplace(dividend, divisor, m = 1, k = 1):\n",
    "  return (dividend + k) / (divisor + (k * m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fff040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data\n",
    "\n",
    "def probability_word_tagged(word, tag, tweets):\n",
    "\n",
    "  n = 0\n",
    "  m = 0\n",
    "\n",
    "  for t in tweets:\n",
    "\n",
    "    if t['tag'] == tag:\n",
    "      m += 1\n",
    "\n",
    "      if word in t['text']:\n",
    "        n += 1\n",
    "\n",
    "  return laplace(n, m, len(tweets))\n",
    "\n",
    "def basic_voting(tweets):\n",
    "\n",
    "  tag = ''\n",
    "  i = 0\n",
    "\n",
    "  tags = [t.get('tag') for t in tweets]\n",
    "\n",
    "  for t in tuple(tags):\n",
    "    n = tags.count(t)\n",
    "    \n",
    "    if i < n:\n",
    "      i = n\n",
    "      tag = t\n",
    "  \n",
    "  return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d00e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms of machine learning\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def basic_naive_bayes(tweet, tweets, c):\n",
    "\n",
    "  max = 0\n",
    "  tag = ''\n",
    "  tags = [t.get('tag') for t in tweets]\n",
    "\n",
    "  for tg in tuple(tags):\n",
    "    prob_tagged = 1\n",
    "\n",
    "    for word in tweet['text']:\n",
    "      prob_tagged = prob_tagged * probability_word_tagged(word, tg, tweets)\n",
    "\n",
    "    prob_tag = tags.count(tg)/len(tags)\n",
    "    prob = prob_tag * prob_tagged\n",
    "\n",
    "    if max < prob:\n",
    "      tag = tg\n",
    "\n",
    "  return tag\n",
    "\n",
    "def basic_kNN(tweet, tweets, k):\n",
    "\n",
    "  res = {}\n",
    "\n",
    "  for text, tag in tweets:\n",
    "    if text != tweet.get('text'):\n",
    "\n",
    "      v = [tag, basic_distance(tweet, {text: tag})]\n",
    "      res['text'] = v\n",
    "  \n",
    "  res = dict(sorted(res.items(), key=lambda item: item[1][1]))\n",
    "  iterate_res = iter(res)\n",
    "\n",
    "  kNN = list(islice(iterate_res, k))\n",
    "\n",
    "  return basic_voting(kNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
