{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. Para leer el fichero de datos y procesarlo, utilizaremos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e578258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1178fa45",
   "metadata": {},
   "source": [
    "Definimos la función que se encarga de leer y procesar el fichero de datos. Esta función eliminará las columnas irrelevantes, seleccionará un subset de 5000 tweets que formará el corpus de nuestros modelos y por último los guardará en otro fichero para su uso más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    #uft-8 encoding didn't work, latin1 encoding did.\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We select a subset of 5000 tweets\n",
    "    data = data[:5000]\n",
    "\n",
    "    #Save data onto new file\n",
    "    data.to_csv(target_filename, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: \n",
      "\n",
      "                                                   text\n",
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "...                                                 ...\n",
      "4995                                    long day today \n",
      "4996                     a friend broke his promises.. \n",
      "4997               @gjarnling I am fine thanks - tired \n",
      "4998          trying to keep my eyes open..damn baking \n",
      "4999                        why the hell is it snowing \n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"subset_tweet_data.csv\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para limpiar, tokenizar y lematizar nuestro dataset.\n",
    "\n",
    "Para esto, descargamos los recursos necesarios para trabajar con la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803a889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bc7c22",
   "metadata": {},
   "source": [
    "Para asegurarnos de que el recurso que contiene las stopwords se descargó correctamente, los mostramos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'where', 'aren', 'that', \"mightn't\", \"shouldn't\", \"won't\", 'through', 'they', 'until', 'don', 'most', 'which', \"wasn't\", \"you've\", 'at', 'who', 'each', 'not', 'having', 'by', 'the', 'both', 'between', 'just', 'd', 'your', 'while', 'too', 'are', 'herself', 'from', 'then', 'under', 'no', 'his', 'any', 're', 'ourselves', 'what', 'some', 'when', 'very', \"hadn't\", 'did', 'so', 'before', 'we', 'again', 'couldn', 'i', 'do', 'was', 'why', 'nor', 'me', \"isn't\", 'into', 'now', 'ours', 'he', \"mustn't\", 'has', \"didn't\", 'because', \"doesn't\", 'same', \"wouldn't\", 's', 'm', 'off', 'but', 'own', 'all', 'as', 'in', 'here', 'about', 'over', 'few', \"you'd\", 'whom', 'up', 'being', 'during', 'hers', 'shouldn', 'my', 'after', \"don't\", 'more', 'those', 'below', 'and', 'a', 'these', 'such', 'an', 'll', 'had', 'hadn', 'should', 'haven', 'needn', 'down', 'other', 'above', 'is', 'once', 've', 'or', \"needn't\", 'were', \"aren't\", 'our', 'on', 'wasn', \"it's\", 'does', 'am', 'them', \"shan't\", 'weren', 'y', 'itself', 'only', 'can', 'didn', 'isn', 'himself', 'shan', 'out', 'wouldn', \"should've\", 'him', \"weren't\", 'against', 'this', 'to', 'mustn', 'mightn', 'doing', 'yourselves', \"hasn't\", 'myself', 'yourself', 'theirs', 'there', 'yours', \"couldn't\", 'doesn', 'will', 'have', 'its', 'be', 'won', 'ain', 'her', 'for', 'of', 'how', 't', \"you're\", 'further', 'than', \"you'll\", 'o', 'hasn', 'it', 'been', 'ma', 'their', 'if', \"haven't\", 'with', \"she's\", 'themselves', 'you', 'she', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d6b52c",
   "metadata": {},
   "source": [
    "Definimos la función encargada de limpiar los tweets del dataset que eliminará las menciones, hashtags, urls y otros símbolos extraños contenidos en ellos.\n",
    "\n",
    "Además después de pasar por este filtro, los tokenizamos con TweetTokenizer que a diferencia del tokenizador por defecto de NLTK, word_tokenizer, este si mantiene la coherencia en palabras complejas del inglés.\n",
    "\n",
    "Por ejemplo la palabra \"can't\":\n",
    "- Con word_tokenizer-> can't = [\"ca\", \"n't\"]\n",
    "- Con TweetTokenize-> can't = [\"can't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d5700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def clean_text(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tt = TweetTokenizer() #Tokenizer\n",
    "    ps = PorterStemmer() #Stemmer\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = data.iloc[i][\"text\"]\n",
    "        filtered_tweet = [word for word in tweet.split() if \n",
    "                                    word.lower()[0].isalnum() and #Eliminate mentions, hashtags\n",
    "                                    word.lower() not in stop_words and #Eliminate stop words\n",
    "                                    'http' not in word.lower()] #Eliminate urls\n",
    "        \n",
    "        filtered_tweet_to_string = ' '.join(str(word) for word in filtered_tweet)\n",
    "\n",
    "        tweet_tokens = tt.tokenize(filtered_tweet_to_string) #Tokenizing\n",
    "\n",
    "        stemmed_tweet_tokens = [ps.stem(token) for token in tweet_tokens] #Stemming\n",
    "        data.loc[i,['text']] = ' '.join(str(token) for token in stemmed_tweet_tokens)\n",
    "\n",
    "    data.to_csv(target_filename, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0ee6ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: \n",
      "\n",
      "                                                   text\n",
      "0     awww , that' bummer . shoulda got david carr t...\n",
      "1     upset can't updat facebook text it ... might c...\n",
      "2     dive mani time ball . manag save 50 % rest go ...\n",
      "3                       whole bodi feel itchi like fire\n",
      "4     no , behav all . i'm mad . here ? can't see th...\n",
      "...                                                 ...\n",
      "4995                                     long day today\n",
      "4996                             friend broke promis ..\n",
      "4997                                    fine thank tire\n",
      "4998                     tri keep eye open .. damn bake\n",
      "4999                                          hell snow\n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_text('subset_tweet_data.csv', 'tokenized_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fff040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data\n",
    "\n",
    "def probability_word_tagged(word, tag, tweets):\n",
    "\n",
    "  tweets_tagged_like = {tw for tw in tweets if tw['tag'] == tag}\n",
    "  n = len(tweets_tagged_like)\n",
    "\n",
    "  tweets_tagged_like_with_word = {tw for tw in tweets if word in tw['text'].split()}\n",
    "  m = len(tweets_tagged_like_with_word)\n",
    "\n",
    "  return m/n\n",
    "\n",
    "def basic_voting(tweets):\n",
    "\n",
    "  tag = ''\n",
    "  i = 0\n",
    "\n",
    "  tags = [t.get('tag') for t in tweets]\n",
    "\n",
    "  for t in tuple(tags):\n",
    "    n = tags.count(t)\n",
    "    \n",
    "    if i < n:\n",
    "      i = n\n",
    "      tag = t\n",
    "  \n",
    "  return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27ed92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic math algorithms\n",
    "\n",
    "def map_algorithm(candidates):\n",
    "\n",
    "  max = {'probability': .0}\n",
    "\n",
    "  for candidate in candidates:\n",
    "    probability = candidate['probability']\n",
    "\n",
    "    if(max['probability'] < probability):\n",
    "      max = candidate\n",
    "\n",
    "  return max\n",
    "\n",
    "def basic_distance(tweet1, tweet2):\n",
    "\n",
    "  distance = 0\n",
    "\n",
    "  text1 = tweet1('text')\n",
    "  text2 = tweet2('text')\n",
    "\n",
    "  for word1 in text1:\n",
    "    for word2 in text2:\n",
    "      if word1 != word2:\n",
    "        distance += 1\n",
    "  \n",
    "  return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d00e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms of machine learning\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def naive_bayes():\n",
    "    return 0\n",
    "\n",
    "def basic_kNN(tweet, tweets, k):\n",
    "\n",
    "  res = {}\n",
    "\n",
    "  for text, tag in tweets:\n",
    "    if text != tweet.get('text'):\n",
    "\n",
    "      v = [tag, basic_distance(tweet, {text: tag})]\n",
    "      res['text'] = v\n",
    "  \n",
    "  res = dict(sorted(res.items(), key=lambda item: item[1][1]))\n",
    "  iterate_res = iter(res)\n",
    "\n",
    "  kNN = list(islice(iterate_res, k))\n",
    "\n",
    "  return basic_voting(kNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
