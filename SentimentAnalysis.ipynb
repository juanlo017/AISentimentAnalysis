{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. Para leer el fichero de datos y procesarlo, utilizaremos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e578258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1178fa45",
   "metadata": {},
   "source": [
    "Definimos la función que se encarga de leer y procesar el fichero de datos. Esta función eliminará las columnas irrelevantes, seleccionará un subset de 5000 tweets que formará el corpus de nuestros modelos y por último los guardará en otro fichero para su uso más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    #uft-8 encoding didn't work, latin1 encoding did.\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We select a subset of 5000 tweets\n",
    "    data = data[:5000]\n",
    "\n",
    "    #Save data onto new file\n",
    "    data.to_csv(target_filename, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: \n",
      "\n",
      "                                                   text\n",
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "...                                                 ...\n",
      "4995                                    long day today \n",
      "4996                     a friend broke his promises.. \n",
      "4997               @gjarnling I am fine thanks - tired \n",
      "4998          trying to keep my eyes open..damn baking \n",
      "4999                        why the hell is it snowing \n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"subset_tweet_data.csv\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para limpiar, tokenizar y lematizar nuestro dataset.\n",
    "\n",
    "Para esto, descargamos los recursos necesarios para trabajar con la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "803a889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jicas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jicas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bc7c22",
   "metadata": {},
   "source": [
    "Para asegurarnos de que el recurso que contiene las stopwords se descargó correctamente, los mostramos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with', \"haven't\", 'ma', 'up', \"aren't\", 'has', 'for', 'any', 'is', 'when', \"needn't\", 'who', 'how', 'after', 'this', 'shouldn', 'its', 'we', 'why', 'hers', 'by', 'will', 'your', 'did', 'and', 'which', 'was', 'theirs', 'ourselves', 'her', 'now', 'these', 'until', 'where', 'wasn', 's', 'during', 'few', 'doesn', 'about', 'against', 'were', 'more', 'most', 'be', 'my', \"should've\", 'before', 'that', 'here', 'just', 'haven', 'isn', \"wouldn't\", \"that'll\", 'should', 'he', 'while', \"isn't\", \"won't\", 'a', 'through', 'me', \"shouldn't\", 'o', 'their', 'from', 'in', \"you'd\", \"hasn't\", \"you'll\", 'because', 've', 'each', 'above', 'didn', 'his', \"you're\", \"wasn't\", 'herself', 'having', 'an', 'some', 'then', 'very', 'doing', 'hasn', 'ain', 'over', 'too', 'needn', 'not', 'shan', 'had', 'our', \"doesn't\", 'himself', 'out', 'once', 'on', 'same', 'won', \"hadn't\", 'but', \"weren't\", \"you've\", 'yourself', 'at', \"it's\", 're', 'mightn', 'are', 'd', 'the', 'couldn', 'can', 'don', 'as', 'wouldn', 'myself', 'being', 'what', 'do', 'yourselves', 'i', 'or', 'am', 'under', 'yours', 'so', \"don't\", 'other', 'below', \"didn't\", 'aren', 'only', 'than', 'm', \"mustn't\", 'hadn', 'nor', 'further', \"shan't\", 'she', 'themselves', 'all', 'it', 'been', 'both', 'off', 'ours', 'into', 'there', 'you', 'itself', \"mightn't\", 'if', 'between', 'down', 'again', 'y', 'such', 'him', \"couldn't\", 'those', 'weren', 't', 'have', 'own', \"she's\", 'whom', 'no', 'll', 'of', 'does', 'they', 'mustn', 'to', 'them'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d6b52c",
   "metadata": {},
   "source": [
    "Definimos la función encargada de limpiar los tweets del dataset que eliminará las menciones, hashtags, urls y otros símbolos extraños contenidos en ellos.\n",
    "\n",
    "Además después de pasar por este filtro, los tokenizamos con TweetTokenizer que a diferencia del tokenizador por defecto de NLTK, word_tokenizer, este si mantiene la coherencia en palabras complejas del inglés.\n",
    "\n",
    "Por ejemplo la palabra \"can't\":\n",
    "- Con word_tokenizer-> can't = [\"ca\", \"n't\"]\n",
    "- Con TweetTokenize-> can't = [\"can't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fa634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "def clean_text(original_filename, target_filename, with_punctuation=True):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mention_or_hashtag_characters = [\"@\", \"#\"]\n",
    "\n",
    "    tt = TweetTokenizer() #Tokenizer\n",
    "    ps = PorterStemmer() #Stemmer\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = data.iloc[i][\"text\"]\n",
    "\n",
    "        tweet_tokens = tt.tokenize(tweet) #Tokenizing\n",
    "\n",
    "        filtered_tweet_tokens = [token for token in tweet_tokens if \n",
    "                                    token.lower()[0] not in mention_or_hashtag_characters and #Eliminate mentions, hashtags\n",
    "                                    token.lower() not in stop_words and #Eliminate stop words\n",
    "                                    'http' not in token.lower()] #Eliminate urls\n",
    "\n",
    "        if(not with_punctuation):\n",
    "            filtered_tweet_tokens = [token for token in filtered_tweet_tokens if token not in list(punctuation)]\n",
    "\n",
    "        stemmed_tweet_tokens = [ps.stem(token) for token in filtered_tweet_tokens] #Stemming\n",
    "        \n",
    "        data.loc[i,['text']] = ' '.join(str(token) for token in stemmed_tweet_tokens)\n",
    "\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fcdd82c",
   "metadata": {},
   "source": [
    "Hacemos 2 llamadas a la función. La primera para el corpus sea texto con signos de puntuación inclusive y otro con texto puramente alfanumérico, esto lo hacemos así para estudiar el posible impacto que puedan tener estos en los modelos finales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa9a83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text('subset_tweet_data.csv', 'tokenized_data.csv')\n",
    "clean_text('subset_tweet_data.csv', 'tokenized_data_no_punctuation.csv', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de10b66a",
   "metadata": {},
   "source": [
    "El siguiente paso consisitirá en utilizar la librería TextBlob para la clasificacion de los tweets en: Muy Feliz, Contento, Neutro, Molesto, Hater.\n",
    "\n",
    "Para ello primero debemos definir los términos y qué consideraremos como 'Neutro', 'Hater', etc..\n",
    "TextBlob mediante su función 'TextBlob.sentiment' nos hace un análisis de sentimientos de cualquier texto en forma de Objeto Sentiment, un objeto que tiene 2 parámetros:\n",
    "\n",
    "    - Polarity con rango [-1,1] que indica la polaridad del sentimiento del texto siendo -1 un texto muy negativo y 1 muy positivo\n",
    "    \n",
    "    - Subjectivity con rango [0,1] que indica la objetividad del texto siendo 0 muy objetivo y cercano a la realidad y 1 siendo muy subjetivo donde se expresan más las opiniones y sentimientos personales del autor.\n",
    "\n",
    "Teniendo en cuenta las limitaciones del procesamiento de análsis de Textblob definiremos:\n",
    "\n",
    "    - Muy Feliz: Polarity=(0.25,1] Subjectivity=[0-0.5)\n",
    "    - Contento: Polarity=(0.25,1] Subjectivity=[0.5-1]\n",
    "    - Neutro: Polarity=(-0.25,0.25] Subjectivity=any\n",
    "    - Hater: Polarity=[-1,-0.25) Subjectivity=[0-0.5)\n",
    "    - Molesto: Polarity=[-1,-0.25) Subjectivity=[0.5-1]\n",
    "    \n",
    "    Esto lo hacemos así ya que consideramos que la principal diferencia entre \"Muy Feliz\"-\"Contento\" y \"Hater\"-\"Molesto\" es que los primeros son opiniones más subjetivas que las segundas. Y por otra parte consideramos que si el texto es muy objetivo decimos que es neutro ya que no expresa sentimiento ninguno de la persona que escribe el tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83070bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def classify_tweets(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    dic_count = dict()\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = TextBlob(str(data.iloc[i][\"text\"]))\n",
    "        sentiment = tweet.sentiment\n",
    "        tag = choose_classification(sentiment)\n",
    "        data.loc[i,['tag']] = tag\n",
    "\n",
    "        if tag not in dic_count:\n",
    "            dic_count[tag] = 1\n",
    "        else:\n",
    "            dic_count[tag] = dic_count[tag] + 1\n",
    "\n",
    "    print(dic_count)\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "\n",
    "def choose_classification(sentiment):\n",
    "    classification = \"Neutral\"\n",
    "    polarity = sentiment.polarity \n",
    "    subjectivity = sentiment.subjectivity \n",
    "    if polarity < -0.25:\n",
    "        classification = \"Molesto\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Hater\"\n",
    "    elif polarity > 0.25:\n",
    "        classification = \"Contento\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Muy Feliz\"\n",
    "    return classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a735976a",
   "metadata": {},
   "source": [
    "Probamos la clasificación de los tweets en los 2 datasets uno con signos de puntuación y otro sin ellos. Podemos ver, por los diccionarios contadores calculados, que devuelven resultados muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2afaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Neutral': 3516, 'Molesto': 656, 'Contento': 596, 'Hater': 80, 'Muy Feliz': 152}\n",
      "{'Neutral': 3467, 'Molesto': 667, 'Contento': 611, 'Hater': 83, 'Muy Feliz': 172}\n"
     ]
    }
   ],
   "source": [
    "classify_tweets(\"tokenized_data_no_punctuation.csv\", \"final_dataset.csv\")\n",
    "classify_tweets(\"tokenized_data.csv\", \"final_dataset_punctuation.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e52a9d81",
   "metadata": {},
   "source": [
    "Tras la revisión manual de los dataset, el siguiente paso es la implementación de distintos algoritmos de aprendizaje automático. Tras una investigación previa, decidimos implementar estos 3 algoritmos:\n",
    "\n",
    "    - Naive Bayes\n",
    "    - Support Vector Machine\n",
    "    - Convolutional Neural Network\n",
    "\n",
    "Otros algoritmos que consideramos fueron; Recursive Neural Network, Logistic Regression y modelos Transformers, decidimos descartar ya que....\n",
    "EXPLICAR POR QUÉ en el documento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b908cc6d",
   "metadata": {},
   "source": [
    "# Seguiremos el siguiente procedimiento para la selección del modelo:\n",
    "\n",
    "#0: Vectorizar el dataset (tf-itf), countVecotrizer es más simple aunque se puede evaluar cual es mejor.\n",
    "\n",
    "#1: Dividir dataset en 5 pliegues de igual longitud\n",
    "\n",
    "#2: Para cada k 1 pliegue sirve de uso para la validación, el resto de entrenamiento. \n",
    "\n",
    "#3: Cuando el modelo esté entrenado, evaluar con el conjunto de evaluación y sacar métricas para evaluar rendimiento.\n",
    "\n",
    "#4: Sacar media de notas de rendimiento del modelo.\n",
    "\n",
    "#5: REPETIR PARA CADA MODELO A IMPLEMENTAR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4ecd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e10157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naiive Bayes a implementar: Gaussian, Multinomial, Complement, Bernoulli\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def tfidf_vectorize_data(original_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    X = data['text'].values.astype('U')\n",
    "    y = data['tag']\n",
    "    #Vectorize data\n",
    "    tag_encoder = LabelEncoder()\n",
    "    vectorizer = TfidfVectorizer(encoding='latin1', ngram_range=(1,2))\n",
    "    vectorized_X = vectorizer.fit_transform(X)\n",
    "    #Encode tags\n",
    "    encoded_y = tag_encoder.fit_transform(y)\n",
    "    return vectorized_X, encoded_y\n",
    "\n",
    "x, y = tfidf_vectorize_data(\"final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b756c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import *\n",
    "\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "complement_nb = ComplementNB()\n",
    "bernoulli_nb = BernoulliNB()\n",
    "categorical_nb = CategoricalNB()\n",
    "\n",
    "nb_models = [multinomial_nb, gaussian_nb, complement_nb, bernoulli_nb, categorical_nb]\n",
    "\n",
    "X, y = tfidf_vectorize_data(\"final_dataset.csv\")\n",
    "#cross validate uses k=5 by default   \n",
    "results = cross_validate(multinomial_nb, X, y, \n",
    "                         return_train_score=True)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ea445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd20e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df55cc8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jicas\\git\\AISentimentAnalysis\\SentimentAnalysis.ipynb Cell 29\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jicas/git/AISentimentAnalysis/SentimentAnalysis.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jicas/git/AISentimentAnalysis/SentimentAnalysis.ipynb#Y151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m gnb \u001b[39m=\u001b[39m GaussianNB()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jicas/git/AISentimentAnalysis/SentimentAnalysis.ipynb#Y151sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m gnb\u001b[39m.\u001b[39;49mfit(X_train, y_train)\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jicas/git/AISentimentAnalysis/SentimentAnalysis.ipynb#Y151sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of mislabeled points out of a total \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m points : \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], (y_test \u001b[39m!=\u001b[39m y_pred)\u001b[39m.\u001b[39msum()))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py:267\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    266\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(y\u001b[39m=\u001b[39my)\n\u001b[1;32m--> 267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    268\u001b[0m     X, y, np\u001b[39m.\u001b[39;49munique(y), _refit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    269\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py:428\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[1;32m--> 428\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[0;32m    429\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:845\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(array):\n\u001b[0;32m    844\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 845\u001b[0m     array \u001b[39m=\u001b[39m _ensure_sparse_format(\n\u001b[0;32m    846\u001b[0m         array,\n\u001b[0;32m    847\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    848\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    849\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    850\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    851\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    852\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    853\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    854\u001b[0m     )\n\u001b[0;32m    855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     \u001b[39m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    858\u001b[0m     \u001b[39m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[39m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:522\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    519\u001b[0m _check_large_sparse(spmatrix, accept_large_sparse)\n\u001b[0;32m    521\u001b[0m \u001b[39mif\u001b[39;00m accept_sparse \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    523\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA sparse matrix was passed, but dense \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata is required. Use X.toarray() to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvert to a dense numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(accept_sparse, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    528\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(accept_sparse) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    " gnb = GaussianNB()\n",
    "\n",
    " y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    " \n",
    " print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4efa95a",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b97bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (5,5,5,5), max_iter = 1000, solver = 'adam')\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a1e1757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.21      0.18       301\n",
      "           1       0.02      0.76      0.04        41\n",
      "           2       0.72      0.08      0.14       338\n",
      "           3       0.00      0.00      0.00        76\n",
      "           4       0.88      0.34      0.49      1744\n",
      "\n",
      "    accuracy                           0.28      2500\n",
      "   macro avg       0.36      0.28      0.17      2500\n",
      "weighted avg       0.73      0.28      0.38      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jicas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jicas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jicas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
