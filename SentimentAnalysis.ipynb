{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e578258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "    print(\"ORIGINAL: \\n\")\n",
    "    print(data)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "    print(\"GIVE COLUMN NAMES: \\n\")\n",
    "    print(data)\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We put the cleaned data into a new csv file\n",
    "    data_cleaned = target_filename\n",
    "    data.to_csv(data_cleaned, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: \n",
      "\n",
      "         0           1                             2         3  \\\n",
      "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...     ..         ...                           ...       ...   \n",
      "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                       4                                                  5  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n",
      "GIVE COLUMN NAMES: \n",
      "\n",
      "         target          id                          date      flag  \\\n",
      "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...         ...         ...                           ...       ...   \n",
      "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                    user                                               text  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n",
      "FINAL RESULT: \n",
      "\n",
      "                                                      text\n",
      "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1        is upset that he can't update his Facebook by ...\n",
      "2        @Kenichan I dived many times for the ball. Man...\n",
      "3          my whole body feels itchy and like its on fire \n",
      "4        @nationwideclass no, it's not behaving at all....\n",
      "...                                                    ...\n",
      "1599995  Just woke up. Having no school is the best fee...\n",
      "1599996  TheWDB.com - Very cool to hear old Walt interv...\n",
      "1599997  Are you ready for your MoJo Makeover? Ask me f...\n",
      "1599998  Happy 38th Birthday to my boo of alll time!!! ...\n",
      "1599999  happy #charitytuesday @theNSPCC @SparksCharity...\n",
      "\n",
      "[1600000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"tweet_cleaned_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para excluir aquellas palabras que no nos aportan ninguna información.\n",
    "Para esto, descargamos los recursos necesarios para trabajar con la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803a889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bc7c22",
   "metadata": {},
   "source": [
    "Para asegurarnos de que el recurso se descargó correctamente los mostramos por pantalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', 'did', \"don't\", 'of', 'those', \"should've\", 'herself', \"doesn't\", 'to', 'further', 'any', 's', 'hasn', 'shouldn', 'am', 'will', 'he', 'some', 'have', 'each', 'themselves', 'hers', 'during', 'under', 'than', 'don', 've', 'mustn', 'ma', \"mightn't\", 'had', 'as', 'against', 'itself', 'an', 'between', 'having', 'into', 'me', 'own', 'we', 'after', 'not', \"needn't\", 'at', 'won', 'shan', 'weren', 'who', 'these', 'haven', \"wouldn't\", 'which', 'theirs', 'doesn', \"mustn't\", 'doing', 'on', \"couldn't\", 'yourselves', 'and', 'ours', 'with', 'now', 'from', 'him', 'my', \"you've\", \"you'll\", 'can', 'very', 'the', 'its', 'ain', 'll', 'through', 'her', 'i', 'how', 're', \"won't\", \"aren't\", 'are', \"shan't\", 'so', 'once', 'you', 'couldn', 'it', 'only', 'isn', 'does', 'she', 'over', 'wasn', 'himself', \"it's\", 'hadn', 'myself', 'm', 'whom', 'here', 'our', 'other', 'y', 'your', 'no', 'd', \"wasn't\", 'ourselves', 'for', 'all', 'in', 'was', 'when', 'being', 'then', \"shouldn't\", \"that'll\", \"she's\", 'if', 'again', 'few', \"weren't\", 'his', 'where', 'them', 'yourself', 'were', \"you'd\", 'both', 'above', 'a', 'such', 'aren', 'been', 'because', 'off', 'until', 'why', 'same', \"hadn't\", 'they', 'before', 'their', 'do', 'more', 'wouldn', 't', 'didn', 'yours', 'too', 'about', 'be', 'below', 'should', \"hasn't\", 'or', 'that', \"didn't\", 'needn', \"you're\", 'o', 'most', 'what', 'while', 'by', 'has', 'nor', 'just', 'is', 'down', 'this', 'mightn', \"isn't\", 'out', \"haven't\", 'there', 'up'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5700f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m tokenized_tweet \u001b[39m=\u001b[39m word_tokenize(tweet)\n\u001b[0;32m     12\u001b[0m filtered_tweet \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokenized_tweet \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[1;32m---> 13\u001b[0m data\u001b[39m.\u001b[39mloc[i,[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m filtered_tweet)\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1797\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1797\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2078\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_check_is_chained_assignment_possible()\n\u001b[0;32m   2077\u001b[0m \u001b[39m# actually do the set\u001b[39;00m\n\u001b[1;32m-> 2078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49msetitem(indexer\u001b[39m=\u001b[39;49mindexer, value\u001b[39m=\u001b[39;49mvalue)\n\u001b[0;32m   2079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:393\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m _using_copy_on_write() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_no_reference(\u001b[39m0\u001b[39m):\n\u001b[0;32m    389\u001b[0m     \u001b[39m# if being referenced -> perform Copy-on-Write and clear the reference\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[39m# this method is only called if there is a single block -> hardcoded 0\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 393\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39msetitem\u001b[39;49m\u001b[39m\"\u001b[39;49m, indexer\u001b[39m=\u001b[39;49mindexer, value\u001b[39m=\u001b[39;49mvalue)\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\juanq\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:986\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[39mif\u001b[39;00m lib\u001b[39m.\u001b[39mis_list_like(vi):\n\u001b[0;32m    983\u001b[0m             \u001b[39m# checking lib.is_scalar here fails on\u001b[39;00m\n\u001b[0;32m    984\u001b[0m             \u001b[39m#  test_iloc_setitem_custom_object\u001b[39;00m\n\u001b[0;32m    985\u001b[0m             casted \u001b[39m=\u001b[39m setitem_datetimelike_compat(values, \u001b[39mlen\u001b[39m(vi), casted)\n\u001b[1;32m--> 986\u001b[0m     values[indexer] \u001b[39m=\u001b[39m casted\n\u001b[0;32m    987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('tweet_cleaned_data.csv', encoding=\"latin1\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # word_tokenize tokeniza las palabras y los caracteres especiales para su fácil procesamiento, por ejemplo:\n",
    "    # \"Hello, Awesome User\" -> ['Hello', ',', 'Awesome', 'User']\n",
    "    tweet = data.iloc[i][\"text\"]\n",
    "    tokenized_tweet = word_tokenize(tweet)\n",
    "    filtered_tweet = [w for w in tokenized_tweet if w.lower() not in stop_words]\n",
    "    data.loc[i,['text']] = ''.join(str(token) for token in filtered_tweet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
