{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. Para leer el fichero de datos y procesarlo, utilizaremos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e578258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1178fa45",
   "metadata": {},
   "source": [
    "Definimos la función que se encarga de leer y procesar el fichero de datos. Esta función eliminará las columnas irrelevantes, seleccionará un subset de 5000 tweets que formará el corpus de nuestros modelos y por último los guardará en otro fichero para su uso más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    #uft-8 encoding didn't work, latin1 encoding did.\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We select a subset of 5000 tweets\n",
    "    data = data[:5000]\n",
    "\n",
    "    #Save data onto new file\n",
    "    data.to_csv(target_filename, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: \n",
      "\n",
      "                                                   text\n",
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "...                                                 ...\n",
      "4995                                    long day today \n",
      "4996                     a friend broke his promises.. \n",
      "4997               @gjarnling I am fine thanks - tired \n",
      "4998          trying to keep my eyes open..damn baking \n",
      "4999                        why the hell is it snowing \n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"subset_tweet_data.csv\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para limpiar, tokenizar y lematizar nuestro dataset.\n",
    "\n",
    "Para esto, descargamos los recursos necesarios para trabajar con la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803a889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jicas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jicas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bc7c22",
   "metadata": {},
   "source": [
    "Para asegurarnos de que el recurso que contiene las stopwords se descargó correctamente, los mostramos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'how', \"you're\", 'aren', 'wasn', 'had', \"wasn't\", \"hasn't\", 'very', 'don', 'was', 'which', 'or', 'the', 'because', 'further', 'against', \"mightn't\", 'up', 'nor', 'himself', 'can', \"weren't\", 'myself', 'that', 'same', 'mustn', 'needn', 'more', 'while', 'been', 'by', 'again', \"hadn't\", 'ourselves', 'to', 'will', 'y', 'll', 'here', 'our', 'didn', \"wouldn't\", 'in', 'then', 'where', 'your', 'mightn', 'weren', 'her', 'is', \"aren't\", 'whom', 'd', 'won', 'yourselves', 'until', 'between', 'hadn', 'hers', 't', \"don't\", 'doesn', 'wouldn', 'why', 'each', 'my', 'o', 'as', 'than', 'do', 'too', 'we', 'on', 'under', 'few', 'some', 'hasn', 'these', 'there', 'doing', 'them', \"shan't\", 'a', 'once', \"you'll\", 'am', 'having', 'theirs', 's', 'only', 'him', \"won't\", 'who', 'she', 'so', 'such', 'his', 'being', 'through', 'other', 'just', 'does', 'of', \"couldn't\", 're', \"isn't\", 'during', 'no', \"you've\", 'themselves', 'should', 'you', 'now', 'but', 'over', 'an', 'ma', 'yours', 'any', \"she's\", 'above', 've', 'both', 'not', 'me', \"should've\", 'were', 'ain', 'after', 'below', 'have', 'it', 'couldn', 'isn', \"shouldn't\", 'those', 'this', \"it's\", \"haven't\", 'all', 'own', \"doesn't\", 'itself', 'and', \"didn't\", 'most', 'shouldn', 'he', 'out', 'they', 'their', 'into', 'off', 'm', 'if', 'when', 'yourself', 'are', 'did', 'with', 'at', \"needn't\", \"you'd\", 'herself', 'be', \"mustn't\", 'from', 'has', 'before', 'its', 'i', 'down', 'haven', 'for', 'shan', \"that'll\", 'what', 'about'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d6b52c",
   "metadata": {},
   "source": [
    "Definimos la función encargada de limpiar los tweets del dataset que eliminará las menciones, hashtags, urls y otros símbolos extraños contenidos en ellos.\n",
    "\n",
    "Además después de pasar por este filtro, los tokenizamos con TweetTokenizer que a diferencia del tokenizador por defecto de NLTK, word_tokenizer, este si mantiene la coherencia en palabras complejas del inglés.\n",
    "\n",
    "Por ejemplo la palabra \"can't\":\n",
    "- Con word_tokenizer-> can't = [\"ca\", \"n't\"]\n",
    "- Con TweetTokenize-> can't = [\"can't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fa634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "def clean_text(original_filename, target_filename, with_punctuation=True):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mention_or_hashtag_characters = [\"@\", \"#\"]\n",
    "\n",
    "    tt = TweetTokenizer() #Tokenizer\n",
    "    ps = PorterStemmer() #Stemmer\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = data.iloc[i][\"text\"]\n",
    "\n",
    "        tweet_tokens = tt.tokenize(tweet) #Tokenizing\n",
    "\n",
    "        filtered_tweet_tokens = [token for token in tweet_tokens if \n",
    "                                    token.lower()[0] not in mention_or_hashtag_characters and #Eliminate mentions, hashtags\n",
    "                                    token.lower() not in stop_words and #Eliminate stop words\n",
    "                                    'http' not in token.lower()] #Eliminate urls\n",
    "\n",
    "        if(not with_punctuation):\n",
    "            filtered_tweet_tokens = [token for token in filtered_tweet_tokens if token not in list(punctuation)]\n",
    "\n",
    "        stemmed_tweet_tokens = [ps.stem(token) for token in filtered_tweet_tokens] #Stemming\n",
    "        \n",
    "        data.loc[i,['text']] = ' '.join(str(token) for token in stemmed_tweet_tokens)\n",
    "\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fcdd82c",
   "metadata": {},
   "source": [
    "Hacemos 2 llamadas a la función. La primera para el corpus sea texto con signos de puntuación inclusive y otro con texto puramente alfanumérico, esto lo hacemos así para estudiar el posible impacto que puedan tener estos en los modelos finales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa9a83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text('subset_tweet_data.csv', 'tokenized_data.csv')\n",
    "clean_text('subset_tweet_data.csv', 'tokenized_data_no_punctuation.csv', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de10b66a",
   "metadata": {},
   "source": [
    "El siguiente paso consisitirá en utilizar la librería TextBlob para la clasificacion de los tweets en: Muy Feliz, Contento, Neutro, Molesto, Hater.\n",
    "\n",
    "Para ello primero debemos definir los términos y qué consideraremos como 'Neutro', 'Hater', etc..\n",
    "TextBlob mediante su función 'TextBlob.sentiment' nos hace un análisis de sentimientos de cualquier texto en forma de Objeto Sentiment, un objeto que tiene 2 parámetros:\n",
    "\n",
    "    - Polarity con rango [-1,1] que indica la polaridad del sentimiento del texto siendo -1 un texto muy negativo y 1 muy positivo\n",
    "    \n",
    "    - Subjectivity con rango [0,1] que indica la objetividad del texto siendo 0 muy objetivo y cercano a la realidad y 1 siendo muy subjetivo donde se expresan más las opiniones y sentimientos personales del autor.\n",
    "\n",
    "Teniendo en cuenta las limitaciones del procesamiento de análsis de Textblob definiremos:\n",
    "\n",
    "    - Muy Feliz: Polarity=(0.25,1] Subjectivity=[0-0.5)\n",
    "    - Contento: Polarity=(0.25,1] Subjectivity=[0.5-1]\n",
    "    - Neutro: Polarity=(-0.25,0.25] Subjectivity=any\n",
    "    - Hater: Polarity=[-1,-0.25) Subjectivity=[0-0.5)\n",
    "    - Molesto: Polarity=[-1,-0.25) Subjectivity=[0.5-1]\n",
    "    \n",
    "    Esto lo hacemos así ya que consideramos que la principal diferencia entre \"Muy Feliz\"-\"Contento\" y \"Hater\"-\"Molesto\" es que los primeros son opiniones más subjetivas que las segundas. Y por otra parte consideramos que si el texto es muy objetivo decimos que es neutro ya que no expresa sentimiento ninguno de la persona que escribe el tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83070bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def classify_tweets(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    dic_count = dict()\n",
    "    for i, row in data.iterrows():\n",
    "        tweet = TextBlob(str(data.iloc[i][\"text\"]))\n",
    "        sentiment = tweet.sentiment\n",
    "        tag = choose_classification(sentiment)\n",
    "        data.loc[i,['tag']] = tag\n",
    "\n",
    "        if tag not in dic_count:\n",
    "            dic_count[tag] = 1\n",
    "        else:\n",
    "            dic_count[tag] = dic_count[tag] + 1\n",
    "\n",
    "    print(dic_count)\n",
    "    data.to_csv(target_filename, index=False)\n",
    "\n",
    "\n",
    "def choose_classification(sentiment):\n",
    "    classification = \"Neutral\"\n",
    "    polarity = sentiment.polarity \n",
    "    subjectivity = sentiment.subjectivity \n",
    "    if polarity < -0.25:\n",
    "        classification = \"Molesto\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Hater\"\n",
    "    elif polarity > 0.25:\n",
    "        classification = \"Contento\"\n",
    "        if subjectivity < 0.5:\n",
    "            classification = \"Muy Feliz\"\n",
    "    return classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a735976a",
   "metadata": {},
   "source": [
    "Probamos la clasificación de los tweets en los 2 datasets uno con signos de puntuación y otro sin ellos. Podemos ver, por los diccionarios contadores calculados, que devuelven resultados muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f2afaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Neutral': 3516, 'Molesto': 656, 'Contento': 596, 'Hater': 80, 'Muy Feliz': 152}\n",
      "{'Neutral': 3467, 'Molesto': 667, 'Contento': 611, 'Hater': 83, 'Muy Feliz': 172}\n"
     ]
    }
   ],
   "source": [
    "classify_tweets(\"tokenized_data_no_punctuation.csv\", \"final_dataset.csv\")\n",
    "classify_tweets(\"tokenized_data.csv\", \"final_dataset_punctuation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb023c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def tfidf_vectorize_data(original_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\")\n",
    "    X = data['text'].values.astype('U')\n",
    "    y = data['tag']\n",
    "    #Vectorize data\n",
    "    tag_encoder = LabelEncoder()\n",
    "    vectorizer = TfidfVectorizer(encoding='latin1', ngram_range=(1,2))\n",
    "    vectorized_X = vectorizer.fit_transform(X)\n",
    "    #Encode tags\n",
    "    encoded_y = tag_encoder.fit_transform(y)\n",
    "    return vectorized_X, encoded_y\n",
    "\n",
    "x, y = tfidf_vectorize_data(\"final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ceb1aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly = SVC(kernel='poly').fit(x,y)\n",
    "linear = SVC(kernel='linear').fit(x,y)\n",
    "rbf = SVC(kernel='rbf').fit(x,y)\n",
    "sigmoid = SVC(kernel='sigmoid').fit(x,y)\n",
    "\n",
    "svc = [poly, linear, rbf, sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92918464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([3.63913822, 3.56732488, 3.70704579, 4.13118839, 4.30142045]), 'score_time': array([0.32513213, 0.31565619, 0.36414528, 0.38479877, 0.35918045]), 'test_score': array([0.706, 0.706, 0.709, 0.705, 0.708]), 'train_score': array([0.99875, 0.99925, 0.99925, 0.999  , 0.999  ])}\n",
      "{'fit_time': array([3.38166475, 3.30865598, 3.20866704, 3.19744515, 3.27841926]), 'score_time': array([0.37142158, 0.3620708 , 0.33238435, 0.33656144, 0.39722133]), 'test_score': array([0.796, 0.788, 0.794, 0.776, 0.782]), 'train_score': array([0.97725, 0.9775 , 0.98025, 0.9755 , 0.97775])}\n",
      "{'fit_time': array([4.27140594, 4.32622552, 4.11885691, 4.14500165, 4.33306837]), 'score_time': array([0.41967392, 0.43762374, 0.44237161, 0.44117904, 0.45264316]), 'test_score': array([0.731, 0.732, 0.726, 0.715, 0.725]), 'train_score': array([0.9595 , 0.958  , 0.9585 , 0.958  , 0.95925])}\n",
      "{'fit_time': array([2.51537585, 2.72932434, 2.46408987, 2.82177043, 3.04160547]), 'score_time': array([0.34807706, 0.33732557, 0.33096147, 0.35079527, 0.3660028 ]), 'test_score': array([0.794, 0.782, 0.79 , 0.773, 0.777]), 'train_score': array([0.928  , 0.9255 , 0.926  , 0.92375, 0.9275 ])}\n"
     ]
    }
   ],
   "source": [
    "res = [cross_validate(i, x, y, return_train_score=True) for i in svc]\n",
    "\n",
    "for r in res:\n",
    "  print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
