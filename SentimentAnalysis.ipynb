{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919521a4",
   "metadata": {},
   "source": [
    "# Proyecto de análisis de sentimientos con Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3550be",
   "metadata": {},
   "source": [
    "Curso 2022/2023:\n",
    "    Juan López Quirós\n",
    "    Jose Ignacio Castro Vázquez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad663c",
   "metadata": {},
   "source": [
    " Lo primero que hay que hacer es escoger o recopilar una primera versión de los datos necesarios que utilizarmos \n",
    " para entrenar a nuestros modelos de Aprendizaje automático. \n",
    " Tras haber estudiado los varios problemas con la API de twitter y las alternativas propuestas, nos decidimos por \n",
    " buscar un dataset ya recopilado de tweets reales de la página web kaggle. En concreto nos decidimos por un dataset\n",
    " ya enfocado al análisis de sentimientos con más de 1.6 millones de tweets recopilados directamente de la API de twitter\n",
    " por lo que se ajusta perfectamente al proyecto.\n",
    "\n",
    " url : https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811715a9",
   "metadata": {},
   "source": [
    " Luego, tenemos que limpiar el dataset escogido para este proyecto.\n",
    " A nosotros solo nos interesa una columna en particular de todo el dataset y ese es la columna de texto, que contiene\n",
    " el contenido de los tweets en sí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fe3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_cleaned_csv(original_filename, target_filename):\n",
    "    data = pd.read_csv(original_filename, encoding=\"latin1\", header=None)\n",
    "    print(\"ORIGINAL: \\n\")\n",
    "    print(data)\n",
    "\n",
    "    #We give the columns a name\n",
    "    column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    data.columns = column_names\n",
    "    print(\"GIVE COLUMN NAMES: \\n\")\n",
    "    print(data)\n",
    "\n",
    "    #We eliminate innecessary columns\n",
    "    data.drop(columns=[\"target\", \"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "    #We put the cleaned data into a new csv file\n",
    "    data_cleaned = target_filename\n",
    "    data.to_csv(data_cleaned, index=False)\n",
    "    print(\"FINAL RESULT: \\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e3af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: \n",
      "\n",
      "         0           1                             2         3  \\\n",
      "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...     ..         ...                           ...       ...   \n",
      "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                       4                                                  5  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n",
      "GIVE COLUMN NAMES: \n",
      "\n",
      "         target          id                          date      flag  \\\n",
      "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...         ...         ...                           ...       ...   \n",
      "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                    user                                               text  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n",
      "FINAL RESULT: \n",
      "\n",
      "                                                      text\n",
      "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1        is upset that he can't update his Facebook by ...\n",
      "2        @Kenichan I dived many times for the ball. Man...\n",
      "3          my whole body feels itchy and like its on fire \n",
      "4        @nationwideclass no, it's not behaving at all....\n",
      "...                                                    ...\n",
      "1599995  Just woke up. Having no school is the best fee...\n",
      "1599996  TheWDB.com - Very cool to hear old Walt interv...\n",
      "1599997  Are you ready for your MoJo Makeover? Ask me f...\n",
      "1599998  Happy 38th Birthday to my boo of alll time!!! ...\n",
      "1599999  happy #charitytuesday @theNSPCC @SparksCharity...\n",
      "\n",
      "[1600000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "create_cleaned_csv(\"tweet_data.csv\", \"tweet_cleaned_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266d73ff",
   "metadata": {},
   "source": [
    "A continuación, utilizamos la libreria NLTK para excluir aquellas palabras que no nos aportan ninguna información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('tweet_cleaned_data.csv')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    # word_tokenize tokeniza las palabras y los caracteres especiales para su fácil procesamiento, por ejemplo:\n",
    "    # \"Hello, Awesome User\" -> ['Hello', ',', 'Awesome', 'User']\n",
    "    tokenized_tweet = word_tokenize(data.iloc[i])\n",
    "    filtered_tweet = [w for w in tokenized_tweet if not w.lower() in stopwords]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"holalaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f971f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"holalaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"holalaaaa\")\n",
    "print(\"holalaaaa\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690aacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
